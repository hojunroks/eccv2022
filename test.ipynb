{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "root = './data/celeba'\n",
    "attributes = pd.read_csv(os.path.join(root, 'list_attr_celeba.csv'))\n",
    "images_dir = os.path.join(root, 'img_align_celeba')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.autoencoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_attribute_threshold = 0.3\n",
    "good_attributes = ['image_id']\n",
    "for key in attributes.keys():\n",
    "    c = attributes[attributes[key]==1]['image_id'].count()\n",
    "    if c < len(attributes)*(1-good_attribute_threshold) and c > len(attributes) * good_attribute_threshold:\n",
    "        good_attributes.append(key)\n",
    "gdf = attributes[good_attributes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image_id', 'Attractive', 'Heavy_Makeup', 'High_Cheekbones', 'Male', 'Mouth_Slightly_Open', 'Smiling', 'Wavy_Hair', 'Wearing_Lipstick']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "list(attributes.keys()).index(\"Mouth_Slightly_Open\")\n",
    "from argparse import Namespace\n",
    "print(good_attributes)\n",
    "print(list(attributes.keys()).index(\"Attractive\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    log_every_n_steps=1,\n",
    "    gradient_clip_val=0.5,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=1e-6,\n",
    "    precision=16,\n",
    "    data_dir = './data/celeba',\n",
    "    batch_size = 512,\n",
    "    num_workers = 4,\n",
    "    gpus=2,\n",
    "    max_epochs=10,\n",
    "    accumulate_grad_batches=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:849: UserWarning: You requested multiple GPUs but did not specify a backend, e.g. `Trainer(strategy=\"dp\"|\"ddp\"|\"ddp2\")`. Setting `strategy=\"ddp_spawn\"` for you.\n",
      "  rank_zero_warn(\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2\n",
      "initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 2 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "  | Name       | Type        | Params\n",
      "-------------------------------------------\n",
      "0 | model      | SimpleCNN   | 10.0 M\n",
      "1 | fc         | Sequential  | 513 K \n",
      "2 | accuracy   | Accuracy    | 0     \n",
      "3 | auroc      | AUROC       | 0     \n",
      "4 | preEncoder | AutoEncoder | 2.7 M \n",
      "-------------------------------------------\n",
      "13.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.1 M    Total params\n",
      "26.278    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 208, in _wrapped_function\n    result = function(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 236, in new_process\n    results = trainer.run_stage()\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1284, in run_stage\n    return self._run_train()\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1306, in _run_train\n    self._run_sanity_check(self.lightning_module)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1370, in _run_sanity_check\n    self._evaluation_loop.run()\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n    self.advance(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 109, in advance\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n    self.advance(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 123, in advance\n    output = self._evaluation_step(batch, batch_idx, dataloader_idx)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 215, in _evaluation_step\n    output = self.trainer.accelerator.validation_step(step_kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 236, in validation_step\n    return self.training_type_plugin.validation_step(*step_kwargs.values())\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 387, in validation_step\n    return self.model(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 886, in forward\n    output = self.module(*inputs[0], **kwargs[0])\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 92, in forward\n    output = self.module.validation_step(*inputs, **kwargs)\n  File \"/home/hjsong/eccv2022/src/models/classifier.py\", line 66, in validation_step\n    loss, accuracy, auroc = self.shared_step(batch, batch_index)\n  File \"/home/hjsong/eccv2022/src/models/classifier.py\", line 49, in shared_step\n    y_hat = self.forward(x)\n  File \"/home/hjsong/eccv2022/src/models/classifier.py\", line 44, in forward\n    x = self.model(x)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/eccv2022/src/models/cnn_for_encoded.py\", line 15, in forward\n    x = self.e1(x)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/eccv2022/src/models/cnn_for_encoded.py\", line 35, in forward\n    x = nn.ReLU()(self.b1(self.c1(x)))\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 446, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 442, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Given groups=1, weight of size [256, 256, 3, 3], expected input[512, 3, 128, 128] to have 256 channels, but got 3 channels instead\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32087/1571909652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCelebAData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_argparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    735\u001b[0m             )\n\u001b[1;32m    736\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 737\u001b[0;31m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[1;32m    738\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \"\"\"\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1272\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_process\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmp_queue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;31m# reset optimizers, since main process is never used for training and thus does not have a valid optim state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(self, function, return_result, *args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spawn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mreturn_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSimpleQueue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_result\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreturn_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_result\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    228\u001b[0m                ' torch.multiprocessing.start_processes(...)' % start_method)\n\u001b[1;32m    229\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart_processes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'spawn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\n-- Process %d terminated with the following error:\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moriginal_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mProcessRaisedException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 0 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/multiprocessing/spawn.py\", line 59, in _wrap\n    fn(i, *args)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 208, in _wrapped_function\n    result = function(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 236, in new_process\n    results = trainer.run_stage()\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1284, in run_stage\n    return self._run_train()\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1306, in _run_train\n    self._run_sanity_check(self.lightning_module)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1370, in _run_sanity_check\n    self._evaluation_loop.run()\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n    self.advance(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py\", line 109, in advance\n    dl_outputs = self.epoch_loop.run(dataloader, dataloader_idx, dl_max_batches, self.num_dataloaders)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 145, in run\n    self.advance(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 123, in advance\n    output = self._evaluation_step(batch, batch_idx, dataloader_idx)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py\", line 215, in _evaluation_step\n    output = self.trainer.accelerator.validation_step(step_kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 236, in validation_step\n    return self.training_type_plugin.validation_step(*step_kwargs.values())\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 387, in validation_step\n    return self.model(*args, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/parallel/distributed.py\", line 886, in forward\n    output = self.module(*inputs[0], **kwargs[0])\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/pytorch_lightning/overrides/base.py\", line 92, in forward\n    output = self.module.validation_step(*inputs, **kwargs)\n  File \"/home/hjsong/eccv2022/src/models/classifier.py\", line 66, in validation_step\n    loss, accuracy, auroc = self.shared_step(batch, batch_index)\n  File \"/home/hjsong/eccv2022/src/models/classifier.py\", line 49, in shared_step\n    y_hat = self.forward(x)\n  File \"/home/hjsong/eccv2022/src/models/classifier.py\", line 44, in forward\n    x = self.model(x)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/eccv2022/src/models/cnn_for_encoded.py\", line 15, in forward\n    x = self.e1(x)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/eccv2022/src/models/cnn_for_encoded.py\", line 35, in forward\n    x = nn.ReLU()(self.b1(self.c1(x)))\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n    return forward_call(*input, **kwargs)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 446, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/home/hjsong/anaconda3/envs/ppml/lib/python3.8/site-packages/torch/nn/modules/conv.py\", line 442, in _conv_forward\n    return F.conv2d(input, weight, bias, self.stride,\nRuntimeError: Given groups=1, weight of size [256, 256, 3, 3], expected input[512, 3, 128, 128] to have 256 channels, but got 3 channels instead\n"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "from src.models.classifier import Classifier\n",
    "from src.datamodule import CelebAData\n",
    "from src.models.cnn_for_encoded import SimpleCNN\n",
    "import pytorch_lightning as pl\n",
    "cnn = SimpleCNN().to('cuda:0')\n",
    "from torchvision.models import resnet34\n",
    "for i in range(len(attributes.keys())-1):\n",
    "    target_attr = i\n",
    "    pre = AutoEncoder.load_from_checkpoint(hparams=args, checkpoint_path='logs/autoencoder/0114/200epochs/adam/version_6/checkpoints/epoch=187-step=15039.ckpt').encoder.eval()\n",
    "    classifier = Classifier(args, model=cnn, target_attr=target_attr, preEncoder=pre)\n",
    "    from pytorch_lightning.loggers import TensorBoardLogger\n",
    "    logger = TensorBoardLogger(\"logs/cls/{}\".format(attributes.keys()[1:][i]), name='')\n",
    "    import sys\n",
    "    sys.argv = ['-f']\n",
    "    parser = ArgumentParser()\n",
    "    parser = pl.Trainer.add_argparse_args(parser)\n",
    "    argu = vars(parser.parse_args())\n",
    "    argu.update(vars(args))\n",
    "    args = Namespace(**argu)\n",
    "    dm = CelebAData(args)\n",
    "    trainer = pl.Trainer.from_argparse_args(args, logger=logger)\n",
    "    trainer.fit(classifier, datamodule=dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "816d599aaaf5094f5c89431f2736cb5ec60c792e143433189ec2e302a3fd18e1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ppml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
